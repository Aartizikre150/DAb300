{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aartizikre150/DAb300/blob/main/Lab_3_Part_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Group - 3\n",
        "\n",
        "Aarti Anil Zikre - 825897\n",
        "\n",
        "Andrews Truman - 824852\n",
        "\n",
        "Premkumar Janakbhai Patel - 829257\n",
        "\n",
        "Vitthlesh Sheth - 825950"
      ],
      "metadata": {
        "id": "pTH2sE8w3sYi"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIT-maBq2bbZ"
      },
      "source": [
        "# Lab 3 Part 2 - Task 1: Parameters in CNN (5 Marks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWo32rVm8TGH"
      },
      "source": [
        "- For the model we have created in **Lab 3 Part 1 Exercise**: Early Stopping with Callbacks, calculate the number of parameters by hand for each layer and compare to the output of model.summary() and print the model summary.\n",
        "- Then print the model summary of **Exercise 7 in Lab 1**\n",
        "- Now compare the Model you created in **Exercise 7 in Lab 1**,\n",
        "  - Compare the Parameters of the models\n",
        "\n",
        "  - Compare Model Performance\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nW4B5rxktgp"
      },
      "source": [
        "**Lab 3 Part 1 Exercise: Early Stopping**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "PWci6yrl2Zlg",
        "outputId": "3dbe2d3e-8eba-4ee7-a278-241cf40186b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n",
            "Epoch 1/20\n",
            "1500/1500 [==============================] - 76s 50ms/step - loss: 0.1535 - accuracy: 0.9536 - val_loss: 0.0800 - val_accuracy: 0.9764\n",
            "Epoch 2/20\n",
            "1500/1500 [==============================] - 76s 51ms/step - loss: 0.0466 - accuracy: 0.9859 - val_loss: 0.0618 - val_accuracy: 0.9833\n",
            "Epoch 3/20\n",
            "1500/1500 [==============================] - 71s 47ms/step - loss: 0.0242 - accuracy: 0.9924 - val_loss: 0.0756 - val_accuracy: 0.9794\n",
            "Epoch 4/20\n",
            "1500/1500 [==============================] - 72s 48ms/step - loss: 0.0151 - accuracy: 0.9952 - val_loss: 0.0748 - val_accuracy: 0.9808\n",
            "Epoch 5/20\n",
            "1500/1500 [==============================] - 71s 47ms/step - loss: 0.0118 - accuracy: 0.9960 - val_loss: 0.0808 - val_accuracy: 0.9816\n",
            "Epoch 6/20\n",
            "1500/1500 [==============================] - 73s 48ms/step - loss: 0.0082 - accuracy: 0.9974 - val_loss: 0.0693 - val_accuracy: 0.9832\n",
            "Epoch 7/20\n",
            "1500/1500 [==============================] - 71s 47ms/step - loss: 0.0070 - accuracy: 0.9974 - val_loss: 0.0872 - val_accuracy: 0.9813\n",
            "Epoch 8/20\n",
            "1500/1500 [==============================] - 72s 48ms/step - loss: 0.0047 - accuracy: 0.9984 - val_loss: 0.0929 - val_accuracy: 0.9833\n",
            "Epoch 9/20\n",
            "1500/1500 [==============================] - 74s 49ms/step - loss: 0.0038 - accuracy: 0.9988 - val_loss: 0.0933 - val_accuracy: 0.9821\n",
            "Epoch 10/20\n",
            "1500/1500 [==============================] - 70s 47ms/step - loss: 0.0042 - accuracy: 0.9987 - val_loss: 0.0895 - val_accuracy: 0.9831\n",
            "Epoch 11/20\n",
            "1500/1500 [==============================] - 70s 46ms/step - loss: 0.0042 - accuracy: 0.9986 - val_loss: 0.1012 - val_accuracy: 0.9828\n",
            "Epoch 12/20\n",
            "1500/1500 [==============================] - 69s 46ms/step - loss: 0.0044 - accuracy: 0.9986 - val_loss: 0.0871 - val_accuracy: 0.9834\n",
            "Epoch 13/20\n",
            "1500/1500 [==============================] - 71s 47ms/step - loss: 0.0018 - accuracy: 0.9994 - val_loss: 0.0935 - val_accuracy: 0.9848\n",
            "Epoch 14/20\n",
            "1500/1500 [==============================] - 68s 45ms/step - loss: 0.0025 - accuracy: 0.9992 - val_loss: 0.1044 - val_accuracy: 0.9825\n",
            "Epoch 15/20\n",
            "1500/1500 [==============================] - 72s 48ms/step - loss: 0.0039 - accuracy: 0.9988 - val_loss: 0.1112 - val_accuracy: 0.9825\n",
            "Epoch 16/20\n",
            "1500/1500 [==============================] - 67s 45ms/step - loss: 0.0022 - accuracy: 0.9993 - val_loss: 0.0962 - val_accuracy: 0.9852\n",
            "Epoch 17/20\n",
            "1500/1500 [==============================] - 71s 47ms/step - loss: 6.3350e-04 - accuracy: 0.9999 - val_loss: 0.1004 - val_accuracy: 0.9855\n",
            "Epoch 18/20\n",
            "1500/1500 [==============================] - 71s 47ms/step - loss: 0.0024 - accuracy: 0.9992 - val_loss: 0.1253 - val_accuracy: 0.9818\n",
            "Epoch 19/20\n",
            "1500/1500 [==============================] - 70s 47ms/step - loss: 0.0028 - accuracy: 0.9992 - val_loss: 0.1166 - val_accuracy: 0.9836\n",
            "Epoch 20/20\n",
            "1500/1500 [==============================] - 73s 49ms/step - loss: 0.0029 - accuracy: 0.9991 - val_loss: 0.1226 - val_accuracy: 0.9828\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 26, 26, 32)        320       \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 21632)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               2769024   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 10)                1290      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2770634 (10.57 MB)\n",
            "Trainable params: 2770634 (10.57 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "# importing the libraries for early stopping\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, Flatten, Dense\n",
        "\n",
        "(x_train,y_train), (x_test,y_test) = mnist.load_data()\n",
        "\n",
        "# Normalizing the values\n",
        "x_train = x_train/255.0\n",
        "x_test = x_test/255.0\n",
        "\n",
        "# This code defines a Sequential model in Keras with a single convolutional layer containing 32 filters of size 3x3, ReLU activation, and an input shape of 28x28x1, followed by a Flatten layer to prepare the data for a dense layer or output.\n",
        "model1 = Sequential([Conv2D(32,(3,3), activation = 'relu', input_shape = (28,28,1)),\n",
        "                    Flatten(),\n",
        "                    Dense(128, activation = 'relu'),\n",
        "                    Dense(10, activation = 'softmax')])\n",
        "model1.compile(optimizer=Adam(learning_rate = 0.001),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# This code initializes an EarlyStopping callback in Keras. It monitors the training loss, waits for 5 consecutive epochs of no improvement, and restores the best weights when training is stopped.\n",
        "early_stopping = EarlyStopping(monitor = 'loss', patience = 5, restore_best_weights = True)\n",
        "history = model1.fit(x_train, y_train, epochs = 20, validation_split = 0.2, callbacks = [early_stopping])\n",
        "\n",
        "\n",
        "print(model1.summary())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FyZ_sJFumjRR"
      },
      "source": [
        "**Exercise 7 in Lab 1**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "OwnMV5lWmi2O",
        "outputId": "f8cc40cb-8056-4181-9eb1-6ffd6dfe9cf0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_2 (Dense)             (None, 512)               401920    \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 512)               0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 128)               65664     \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 128)               16512     \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 10)                1290      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 485386 (1.85 MB)\n",
            "Trainable params: 485386 (1.85 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from keras.layers import Dropout\n",
        "\n",
        "network2 = Sequential()\n",
        "network2.add(Dense(512, activation='sigmoid', input_shape=(28 * 28,)))\n",
        "# Added a Dropout layer with a specified dropout rate\n",
        "network2.add(Dropout(0.5))\n",
        "network2.add(Dense(128, activation='sigmoid'))\n",
        "network2.add(Dense(128, activation='relu'))\n",
        "# Added a Dropout layer with a specified dropout rate\n",
        "network2.add(Dropout(0.5))\n",
        "network2.add(Dense(10, activation='softmax')) # Multi clasiification problem\n",
        "\n",
        "network2.summary()\n",
        "\n",
        "network2.compile(optimizer='rmsprop',\n",
        "                loss='categorical_crossentropy',\n",
        "                metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "maXaHAcw2TsB"
      },
      "source": [
        "# Lab 3 Part 2 - Task 2: CIFAR-10 Challenge (10 Marks)\n",
        "\n",
        "In this lab you will experiment with whatever ConvNet architecture/design you'd like on [CIFAR-10 image dataset](https://www.cs.toronto.edu/~kriz/cifar.html).\n",
        "\n",
        "\n",
        "## Exercise  1: Creating the network\n",
        "\n",
        "**Goal:** After training, your model should achieve **at least 80%** accuracy on a **validation** set within 20 epochs. (Or as close as possible as long as there is demonstrated effort to achieve this goal.)\n",
        "\n",
        "**Data split** The training set should consist of 40000 images, the validation set should consist of 10000 images, and the test set should consist of the remaining 10000 images. **Please use the Keras `load_data()` function to import the data set.**\n",
        "\n",
        "\n",
        "### Some things you can try:\n",
        "- Different number/type of layers\n",
        "- Different filter sizes\n",
        "- Adjust the number of filters used in any given layer\n",
        "- Try various pooling strategies\n",
        "- Consider using batch normalization\n",
        "- Check if adding regularization helps\n",
        "- Consider alternative optimizers\n",
        "- Try different activation functions\n",
        "\n",
        "\n",
        "### Tips for training\n",
        "When building/tuning your model, keep in mind the following points:\n",
        "\n",
        "- This is experimental, so be driven by results achieved on the validation set as opposed to what you have heard/read works well or doesn't\n",
        "- If the hyperparameters are working well, you should see improvement in the loss/accuracy within approximately one epoch\n",
        "- For hyperparameter tuning: start by testing a large range of hyperparameters for just a few training iterations to find the combinations of parameters that are working at all\n",
        "- Once you have found some sets of hyperparameters that seem to work, search more finely around these parameters. You may need to train for more epochs.\n",
        "- Prefer random search to grid search for hyperparameters\n",
        "- You should use the validation set for hyperparameter search and for evaluating different architectures\n",
        "- The test set should only be used at the very end to evaluate your final model\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Load the CIFAR-10 dataset\n",
        "(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()\n",
        "\n",
        "# Split the data into training, validation, and test sets\n",
        "val_split = 0.1  # 10% for validation\n",
        "val_size = int(train_images.shape[0] * val_split)\n",
        "val_images, val_labels = train_images[:val_size], train_labels[:val_size]\n",
        "train_images, train_labels = train_images[val_size:], train_labels[val_size:]\n",
        "\n",
        "# Normalize the pixel values to be between 0 and 1\n",
        "train_images, val_images, test_images = train_images / 255.0, val_images / 255.0, test_images / 255.0\n",
        "\n",
        "# One-hot encode the labels\n",
        "train_labels = to_categorical(train_labels, num_classes=10)\n",
        "val_labels = to_categorical(val_labels, num_classes=10)\n",
        "test_labels = to_categorical(test_labels, num_classes=10)\n",
        "\n",
        "# Define the model\n",
        "model = models.Sequential()\n",
        "\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu'))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Dropout(0.25))\n",
        "\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Dropout(0.25))\n",
        "\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(512, activation='relu'))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.Dropout(0.5))\n",
        "model.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(train_images, train_labels, epochs=20, batch_size=64, validation_data=(val_images, val_labels))\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_accuracy = model.evaluate(test_images, test_labels)\n",
        "print(\"Test Accuracy: {:.2f}%\".format(test_accuracy * 100))\n"
      ],
      "metadata": {
        "id": "RONI-s873Yka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64BbG8Y62TsH"
      },
      "source": [
        "## Exercise 2: Describe What you did\n",
        "\n",
        "All the work you did leading up to your final model should be summarized in this section. This should be a logical and well-organized summary of the various experiments that were tried in **Lab 3 Part 2 - Task 2:Exercise 1**, and should be captured in **table format**. Upon reading this section I should understand what you tried, the reasoning behind trying it, any quantitative values that correspond to what you tried, and the results.\n",
        "\n",
        "See [this guide](https://www.datacamp.com/community/tutorials/markdown-in-jupyter-notebook) for how to format markdown cells in Jupyter notebooks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sa8p0Nkj2TsI"
      },
      "source": [
        "| Experiment          | Changes Made                                                           | Reasoning                                        | Quantitative Values                      | Results                                               |\n",
        "|---------------------|-----------------------------------------------------------------------|--------------------------------------------------|------------------------------------------|-------------------------------------------------------|\n",
        "| Baseline Model      | Used a basic Convolutional Neural Network (CNN) architecture with two convolutional layers, max-pooling, and fully connected layers. | Initial model to establish a baseline performance. | Training accuracy and validation accuracy. | Baseline model achieved reasonable accuracy but was overfitting. |\n",
        "| Data Preprocessing  | Normalized pixel values to be between 0 and 1.                        | Standard preprocessing to scale input data.      | None specified.                        | Data preprocessing improved model convergence. |\n",
        "| Batch Normalization | Added Batch Normalization layers after each convolutional and fully connected layer. | To improve training stability and reduce overfitting. | Training accuracy and validation accuracy. | Batch Normalization helped the model to converge faster and improved generalization. |\n",
        "| Increased Model Complexity | Added more convolutional layers, dropout layers, and increased the number of neurons in the fully connected layers. | To capture more complex features and reduce overfitting. | Training accuracy and validation accuracy. | Increased model complexity improved both training and validation accuracy. |\n",
        "| Hyperparameter Tuning | Adjusted hyperparameters such as batch size, number of epochs, and learning rate. | To find the best set of hyperparameters for training. | Training accuracy, validation accuracy, and loss. | Hyperparameter tuning improved model performance. |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Final Model and Performance:\n",
        "\n",
        "Our final model consists of multiple convolutional layers, Batch Normalization, dropout layers, and fully connected layers. It was trained for 20 epochs with a batch size of 64 using the Adam optimizer. The model achieved a test accuracy of 79.62%.\n",
        "\n",
        "Overall Results:\n",
        "\n",
        "The experiments led to a significant improvement in model performance compared to the baseline.\n",
        "Data preprocessing, Batch Normalization, and increased model complexity contributed to better results.\n",
        "Careful hyperparameter tuning helped in achieving the best model performance."
      ],
      "metadata": {
        "id": "NhW-zaGI3e9Z"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oqbGQ2343czQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}